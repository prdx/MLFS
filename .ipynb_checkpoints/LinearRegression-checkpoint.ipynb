{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anak Agung Ngurah Bagus Trihatmaja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Write a code in Python whose input is a training dataset {(x1, y1), . . . , (xN , yN )} and its output is the weight vector θ in the linear regression model y = θ'φ(x), for a given nonlinear mapping φ(·). Implement two cases: i) using the closed-form solution, ii) using a stochastic gradient descent on mini-batches of size m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import scipy.io\n",
    "import math\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Load the dataset\n",
    "df = scipy.io.loadmat(\"dataset1.mat\")\n",
    "df2 = scipy.io.loadmat(\"dataset2.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data based on the formula we input \n",
    "# For this time we will generate a square function added with a noise\n",
    "f = lambda  x: x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed function regression\n",
    "def closed_function_regression(train_X, train_Y):\n",
    "  X = np.array(train_X)\n",
    "  y = np.array(train_Y)\n",
    "  \n",
    "        \n",
    "  Xt = X.T\n",
    "  product = np.dot(Xt, X)\n",
    "  theInverse = inv(product)\n",
    "  theta = np.dot(np.dot(theInverse, Xt), y)\n",
    "        \n",
    "  return theta  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of this function: http://adventuresinmachinelearning.com/stochastic-gradient-descent/\n",
    "def shuffle_data(train_X, train_Y, batch_size):\n",
    "    X = train_X\n",
    "    y = train_Y\n",
    "    \n",
    "    random_indices = np.random.choice(len(X), len(y), replace=False)\n",
    "    \n",
    "    X_shuffled = X[random_indices,:]\n",
    "    y_shuffled = y[random_indices]\n",
    "    mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_gradient_descent(train_X, train_Y, learning_rate, num_iter, batch_size):\n",
    "    # Prepare the data\n",
    "    X = np.array(train_X)\n",
    "    y = np.array(train_Y)\n",
    "    \n",
    "    # Randomize the data and split into samples\n",
    "    df = shuffle_data(X, y, batch_size)\n",
    "    \n",
    "    # Get the dimension\n",
    "    x_col = X.shape[1]\n",
    "    \n",
    "    # Set the initial theta and other initial variables\n",
    "    theta = np.zeros((x_col, 1))\n",
    "    \n",
    "    start_i = 0\n",
    "    \n",
    "    # Create the loop\n",
    "    for i in range(start_i + 1, num_iter + 1):\n",
    "        # For each randomized-mini-batch partition\n",
    "        for j in range(0, len(df)):\n",
    "            X = df[j][0]\n",
    "            y = df[j][1]\n",
    "            \n",
    "            y_hat = np.dot(X, theta)\n",
    "            \n",
    "            # Break if overflow occurs\n",
    "            theta = theta - learning_rate * np.dot(X.T, y_hat - y)\n",
    "            \n",
    "            # FIXME: Break if NaN occurs\n",
    "            # if not (float('-inf') < float(temp[0]) < float('inf')): \n",
    "            #    return theta\n",
    "                \n",
    "            # theta =  temp\n",
    "            \n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Consider n-degree polynomials, φ(·) =  1 x x^2 · · · x^n . Download the dataset on the course webpage and work with ‘dataset1’. Run the code on the training data to compute θ for n ∈ {2, 3, 5}. Evaluate the regression error on both training and the test data. Report θ, training error and test error for both implementation (closed-form vs gradient descent). What is the effect of the size of the mini-batch on the speed and testing error of the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.40148277]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_trn = df['X_trn']\n",
    "Y_trn = df['Y_trn']\n",
    "X_test = df['X_tst']\n",
    "Y_test = df['Y_tst']\n",
    "\n",
    "\n",
    "closed_function_regression(X_trn, Y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_polinomial_array(element, n):\n",
    "    pol = np.empty([1, n+1])\n",
    "    # Debug\n",
    "    # print(pol)\n",
    "    for i in range(0, n+1):\n",
    "        # Debug\n",
    "        # print(\"Element: \\n\", element)\n",
    "        pol[0][i] = element ** i\n",
    "        \n",
    "    return pol[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.24652623]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we set the degree of polynomial        \n",
    "degree = 5\n",
    "# degree + 1 for 1's in the first column\n",
    "X_trn_new = np.ndarray((len(X_trn), degree + 1))\n",
    "\n",
    "# print(X_trn_new)\n",
    "\n",
    "i = 0\n",
    "X = np.array(X_trn)\n",
    "for x in X:\n",
    "    X_trn_new[i] = create_polinomial_array(x, degree)\n",
    "    i += 1\n",
    "        \n",
    "\n",
    "mini_gradient_descent(X_trn, Y_trn, 0.001, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing the regression error and speed\n",
    "# Hyper-parameter are the alpha (learning rate), the batch size and the number of iterations\n",
    "def analyze(learning_rate, n_iteration, batch_size):\n",
    "    degrees = [2, 3, 5]\n",
    "    \n",
    "    # Time for performance\n",
    "    grad_desc_performance = []\n",
    "    \n",
    "    # For train data\n",
    "    error_closed_form_train = []\n",
    "    error_grad_desc_train = []\n",
    "    \n",
    "    # For test data\n",
    "    error_closed_form_test = []\n",
    "    error_grad_desc_test = []\n",
    "    \n",
    "    X = np.array(X_trn)\n",
    "    X_tst = np.array(X_test)\n",
    "    \n",
    "    for degree in degrees:\n",
    "        X_trn_new = np.ndarray((len(X_trn), degree + 1))\n",
    "        X_tst_new = np.ndarray((len(X_tst), degree + 1))\n",
    "        \n",
    "        i = 0\n",
    "        for x in X:\n",
    "            X_trn_new[i] = create_polinomial_array(x, degree)\n",
    "            i += 1\n",
    "        \n",
    "        i = 0\n",
    "        for x in X_tst:\n",
    "            X_tst_new[i] = create_polinomial_array(x, degree)\n",
    "            i += 1\n",
    "        \n",
    "        tetha_closed_form = closed_function_regression(X_trn_new, Y_trn)\n",
    "        t = time.process_time()\n",
    "        tetha_grad_desc = mini_gradient_descent(X_trn_new, Y_trn, learning_rate, n_iteration, batch_size)\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print(\"Theta for degree {} done in: {}\\n\".format(degree, elapsed_time))\n",
    "        \n",
    "        \n",
    "        y_hat_closed_form_train = np.dot(X_trn_new, tetha_closed_form)\n",
    "        y_hat_grad_desc_train = np.dot(X_trn_new, tetha_grad_desc)\n",
    "        \n",
    "        y_hat_closed_form_test = np.dot(X_tst_new, tetha_closed_form)\n",
    "        y_hat_grad_desc_test = np.dot(X_tst_new, tetha_grad_desc)\n",
    "        \n",
    "        error_closed_form_train.append(np.sqrt(((Y_trn - y_hat_closed_form_train) ** 2).mean()))\n",
    "        error_grad_desc_train.append(np.sqrt(((Y_trn - y_hat_grad_desc_train) ** 2).mean()))\n",
    "        \n",
    "        error_closed_form_test.append(np.sqrt(((Y_test - y_hat_closed_form_test) ** 2).mean()))\n",
    "        error_grad_desc_test.append(np.sqrt(((Y_test - y_hat_grad_desc_test) ** 2).mean()))\n",
    "        \n",
    "    d = {\n",
    "        'degree': [2, 3, 5],\n",
    "        'close_form_error_train': error_closed_form_train,\n",
    "        'grad_desc_error_train': error_grad_desc_train,\n",
    "        'close_form_error_test': error_closed_form_test,\n",
    "        'grad_desc_error_test': error_grad_desc_test\n",
    "    }\n",
    "    errors = pd.DataFrame(data = d)\n",
    "    return errors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta for degree 2 done in: 0.006700000000002149\n",
      "\n",
      "Theta for degree 3 done in: 0.0014400000000023283\n",
      "\n",
      "Theta for degree 5 done in: 0.0012510000000034438\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.542162e+01</td>\n",
       "      <td>1.651324e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>1.908523e+01</td>\n",
       "      <td>1.169451e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>2.331942e+91</td>\n",
       "      <td>8.971256e+89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.542162e+01</td>\n",
       "      <td>1.651324e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>1.908523e+01</td>\n",
       "      <td>1.169451e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>2.331942e+91</td>\n",
       "      <td>8.971256e+89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size 10\n",
    "result1 = analyze(0.00001, 10, 10)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta for degree 2 done in: 0.001145999999998537\n",
      "\n",
      "Theta for degree 3 done in: 0.0009610000000037644\n",
      "\n",
      "Theta for degree 5 done in: 0.0008180000000024279\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.064661e+01</td>\n",
       "      <td>1.862111e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>2.641661e+01</td>\n",
       "      <td>1.402642e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>9.985755e+44</td>\n",
       "      <td>3.666089e+43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.064661e+01</td>\n",
       "      <td>1.862111e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>2.641661e+01</td>\n",
       "      <td>1.402642e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>9.985755e+44</td>\n",
       "      <td>3.666089e+43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size 20\n",
    "result1 = analyze(0.00001, 6, 20)\n",
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the hyperparameter of `number of iteration` is 10 and `learning rate` is 0.00001 and we split the data by 10, so we get 12 partition. We get smaller RMSE compared to the closed form for our training data.\n",
    "\n",
    "The error we get varies depends on our hyperparameters we mention above. In case of polinomial of 5, we get better result if we increase the batch size and adjust the number of iteration accordingly.\n",
    "\n",
    "With batch size of 10, we get the time decreasing from degree 2, 3 to 5 but for batch size of 20, the time is similar, except for degree 2. Therefore, we can conclude there is no significant influence of the batch time to the performance in this case.\n",
    "\n",
    "We have known bug that if we set the learning rate or the number of iteration too high, we will overflow exception (known bugs are marked as `# FIXME` in this report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Download the dataset on the course webpage and work with ‘dataset2’. Write a code in Python that applies Ridge regression to the dataset to compute θ for a given λ. Implement two cases:\n",
    " 2\n",
    "using a closed-form solution and using a stochastic gradient descent method with mini-batches of size m. Use K-fold cross validation on the training dataset to obtain the best regularization λ and apply the optimal θ to compute the regression error on test samples. Report the optimal λ, θ, test and training set errors for K ∈ {2,10,N}, where N is the number of samples. In all cases try n ∈ {2, 3, 5}. How does the test error change as a function of λ and n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(object):\n",
    "    def __init__(self, lmbda=0.1):\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        C = X.T.dot(X) + self.lmbda * np.eye(X.shape[1])\n",
    "        self.w = np.linalg.inv(C).dot(X.T.dot(y))\n",
    "    \n",
    "    def set_params(self, lmbda=0.1):\n",
    "        self.lmbda = lmbda\n",
    "        return self\n",
    "        \n",
    "    def get_weight(self):\n",
    "        return self.w\n",
    "    \n",
    "    # def get_error(self):\n",
    "    #     y_hat = np.dot(self.X, self.w)\n",
    "    #     return np.sqrt(((self.Y - y_hat) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimisation(object):\n",
    "    def __init__(self, X, y, model):\n",
    "        self.model = model\n",
    "        # Prepare the data\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        \n",
    "    def closed_form(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        return self.model.get_weight()\n",
    "    \n",
    "    def mini_batch_stochastic(self, learning_rate, num_iter, batch_size):\n",
    "        # Randomize the data and split into samples\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        \n",
    "        df = shuffle_data(X, y, batch_size)\n",
    "    \n",
    "        # Get the dimension\n",
    "        x_dim = X.shape[1]\n",
    "    \n",
    "        # Set the initial tetha and other initial variables\n",
    "        theta = np.zeros((x_dim, 1))\n",
    "    \n",
    "        start_i = 0\n",
    "    \n",
    "        # Create the loop\n",
    "        for i in range(start_i + 1, num_iter + 1):\n",
    "            # For each randomized-mini-batch partition\n",
    "            for j in range(0, len(df)):\n",
    "                X = df[j][0]\n",
    "                y = df[j][1]\n",
    "                \n",
    "                self.model.fit(self.X, self.y)\n",
    "                theta = theta - learning_rate * self.model.get_weight() \n",
    "        \n",
    "        return theta\n",
    "        \n",
    "    def shuffle_data(self, batch_size):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "    \n",
    "        random_indices = np.random.choice(len(X), len(y), replace=False)\n",
    "    \n",
    "        X_shuffled = X[random_indices,:]\n",
    "        y_shuffled = y[random_indices]\n",
    "        mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "        return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "X_trn2 = df2['X_trn']\n",
    "Y_trn2 = df2['Y_trn']\n",
    "\n",
    "\n",
    "# Load test data\n",
    "X_test2 = df2['X_tst']\n",
    "Y_test2 = df2['Y_tst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X, y, lmbda, fold):\n",
    "    # Split the data into fold part\n",
    "    # Prepare the data\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    ridge = RidgeRegression()\n",
    "    \n",
    "    # Randomize the data and split into samples\n",
    "    batch_size = int(len(X) / fold)\n",
    "    df = shuffle_data(X, y, batch_size)\n",
    "    \n",
    "    test_result = []\n",
    "    \n",
    "    for i in range(0, len(df)):\n",
    "        # Temporary data for k-fold\n",
    "        temp = df.copy()\n",
    "        \n",
    "        # To store partitions for training\n",
    "        train = []\n",
    "        \n",
    "        # Save and pop an element for test\n",
    "        test = temp.pop(i)\n",
    "        \n",
    "        # Concat the remaining array\n",
    "        X_train = np.empty(shape=[X.shape[0], X.shape[1]])\n",
    "        Y_train = np.empty(shape=[y.shape[0], y.shape[1]])\n",
    "        \n",
    "        for j in range(0, len(df)):\n",
    "            _X = df[j][0]\n",
    "            _y = df[j][1]\n",
    "            X_train = np.concatenate((X_train, _X))\n",
    "            Y_train = np.concatenate((Y_train, _y))\n",
    "        \n",
    "        \n",
    "        ridge.set_params(lmbda)\n",
    "        # test[0] is X in test data\n",
    "        # test[1] is Y in test data\n",
    "        \n",
    "                            \n",
    "        ridge.fit(X_train, Y_train)\n",
    "        #print(X_train, Y_train)\n",
    "        w = ridge.get_weight()\n",
    "        \n",
    "        if ~(math.isnan(w[0])):\n",
    "            y_hat = np.dot(test[0], w)\n",
    "            test_result.append(np.sqrt(((test[1] - y_hat) ** 2).mean()))\n",
    "    \n",
    "    return np.nanmean(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_analysis(degree, fold):\n",
    "    X = np.array(X_trn2)\n",
    "    k_fold_result = []\n",
    "    \n",
    "    X_trn_new = np.ndarray((len(X_trn2), degree + 1))\n",
    "    i = 0\n",
    "    for x in X:\n",
    "        X_trn_new[i] = create_polinomial_array(x, degree)\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    for j in range(-10, 2):\n",
    "        k_fold_result.append(k_fold(X_trn_new, Y_trn2, 10 ** j, fold))\n",
    "    \n",
    "    return k_fold_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:48: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial degree: 2, K-Fold: 2\n",
      "\n",
      "Minimum error at index: 1 value nan with lambda: 10^-9\n",
      "\n",
      "Polynomial degree: 2, K-Fold: 10\n",
      "\n",
      "Minimum error at index: 5 value nan with lambda: 10^-5\n",
      "\n",
      "Polynomial degree: 2, K-Fold: 20\n",
      "\n",
      "Minimum error at index: 10 value 39.486641916240735 with lambda: 10^0\n",
      "\n",
      "Polynomial degree: 3, K-Fold: 2\n",
      "\n",
      "Minimum error at index: 0 value 21.882242335868636 with lambda: 10^-10\n",
      "\n",
      "Polynomial degree: 3, K-Fold: 10\n",
      "\n",
      "Minimum error at index: 6 value 25.514165069952902 with lambda: 10^-4\n",
      "\n",
      "Polynomial degree: 3, K-Fold: 20\n",
      "\n",
      "Minimum error at index: 2 value 26.423533525400718 with lambda: 10^-8\n",
      "\n",
      "Polynomial degree: 5, K-Fold: 2\n",
      "\n",
      "Minimum error at index: 7 value 15.189549233205817 with lambda: 10^-3\n",
      "\n",
      "Polynomial degree: 5, K-Fold: 10\n",
      "\n",
      "Minimum error at index: 0 value 23.077002094064518 with lambda: 10^-10\n",
      "\n",
      "Polynomial degree: 5, K-Fold: 20\n",
      "\n",
      "Minimum error at index: 11 value 20.25594287743636 with lambda: 10^1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "degrees = [2, 3, 5]\n",
    "folds = [2, 10, 20]\n",
    "\n",
    "for degree in degrees:\n",
    "    for fold in folds:\n",
    "        temp = k_fold_analysis(degree, fold)\n",
    "        print(\"Polynomial degree: {}, K-Fold: {}\\n\".format(degree, fold))\n",
    "        print(\"Minimum error at index: {} value {} with lambda: 10^{}\\n\".format(\n",
    "            np.argmin(temp), np.min(temp), (np.argmin(temp) - 10)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we see that the best lambda is depending upon the number of parameters in X--polynomial with degree 5 has different lambda that result the smallest error compared to other polynomial degree. With the values above, let's try to compute the weight value using closed form and stochastic. \n",
    "\n",
    "Note: The value can change since we shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_closed_form_test = []\n",
    "error_grad_desc_test = []\n",
    "\n",
    "X = np.array(X_trn2)\n",
    "X_tst = np.array(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For degree 2\n",
    "degree = 2\n",
    "X_trn_new = np.ndarray((len(X_trn2), degree + 1))\n",
    "X_tst_new = np.ndarray((len(X_test2), degree + 1))\n",
    "        \n",
    "i = 0\n",
    "for x in X:\n",
    "    X_trn_new[i] = create_polinomial_array(x, degree)\n",
    "    i += 1\n",
    "    \n",
    "i = 0\n",
    "for x in X_tst:\n",
    "    X_tst_new[i] = create_polinomial_array(x, degree)\n",
    "    i += 1\n",
    "\n",
    "ridge = RidgeRegression()\n",
    "ridge.set_params(10**-9)\n",
    "m = Minimisation(X_trn_new, Y_trn2, ridge)\n",
    "\n",
    "w1 = m.closed_form()\n",
    "y_hat1 = np.dot(X_tst_new, w1)\n",
    "e1 = np.sqrt(((Y_test2 - y_hat1) ** 2).mean())\n",
    "error_closed_form_test.append(e1)\n",
    "\n",
    "w2 = m.mini_batch_stochastic(0.00001, 10, 10)\n",
    "y_hat2 = np.dot(X_tst_new, w2)\n",
    "e2 = np.sqrt(((Y_test2 -  y_hat2) ** 2).mean())\n",
    "error_grad_desc_test.append(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close_form_error_test': [41.791571352723821],\n",
       " 'degree': [2],\n",
       " 'grad_desc_error_test': [103.78852572764892]}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "        'degree': [2],\n",
    "        'close_form_error_test': error_closed_form_test,\n",
    "        'grad_desc_error_test': error_grad_desc_test\n",
    "    }\n",
    "\n",
    "# Print the error in close form and gradient descent using the theta we get from the close form\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
